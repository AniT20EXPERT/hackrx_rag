
# ğŸ§  HackRx RAG API â€” Document Q&A using LLaMA-3 + Pinecone

This project is a production-ready **Retrieval-Augmented Generation (RAG) API** that takes in a user-uploaded PDF and a list of natural language questions, performs contextual retrieval over the document, and returns accurate answers powered by **Groqâ€™s ultra-fast LLaMA-3.1-70B Versatile model**.

---

## ğŸš€ Features

- ğŸ“„ Accepts any publicly accessible PDF document
- â“ Accepts multiple user questions in one request
- ğŸ§© Uses **sentence-aware chunking** (NLTK) to split documents at sentence boundaries
- ğŸ“Œ Uses **hash-based chunk IDs** to prevent duplicate embeddings of the same document
- ğŸ’¾ Stores and retrieves embeddings via **Pinecone vector database**
- âš¡ Embeddings are generated **locally using Huggingface Sentence-Transformers**
- ğŸ¤– Question answering powered by **LLaMA-3.1-70B Versatile via Groq API**

---

## ğŸ› ï¸ Tech Stack

| Layer                  | Tool/Service                             |
|------------------------|-------------------------------------------|
| âš™ Backend              | FastAPI                                   |
| ğŸ“„ Embeddings          | `sentence-transformers` (local, Huggingface) |
| ğŸ” Vector Store        | [Pinecone](https://www.pinecone.io/)      |
| ğŸ§  LLM (Answering)     | [`LLaMA-3.1-70B Versatile`](https://console.groq.com/) via Groq API |
| ğŸ§  Sentence Chunking   | [NLTK](https://www.nltk.org/) for sentence splitting |
| ğŸ” Auth                | Bearer token-based authorization          |
| ğŸ”„ Deduplication       | SHA256 hash of document + chunk IDs       |

---

## ğŸ“¦ API Overview

### ğŸ”¹ `POST /hackrx/run`

#### Request Example:
```json
{
  "documents": "https://example.com/your-policy.pdf",
  "questions": [
    "What is the grace period for premium payment?",
    "What are the exclusions under maternity benefits?"
  ]
}
````

#### Header:

```
Authorization: Bearer <your_api_key>
```

#### Response Example:

```json
{
  "answers": [
    "A 30-day grace period is provided for premium payments.",
    "Maternity benefits are covered after 24 months with sub-limits."
  ]
}
```

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ main1.py                  # FastAPI entrypoint
â”œâ”€â”€ get_text_pdf.py         # PDF text extraction logic
â”œâ”€â”€ get_embeddings.py       # Sentence splitting + local embedding generation
â”œâ”€â”€ llm_answering.py        # Groq API answering using LLaMA-3
â”œâ”€â”€ pinecone_db.py          # Pinecone upsert/query utilities
â”œâ”€â”€ hash_text.py            # SHA256 hashing for deduplication
â”œâ”€â”€ .env                    # Store your API keys here
```

---

## ğŸ” Environment Variables

Create a `.env` file in your project root with the following content:

```env
MY_API_KEY_FOR_AUTH=your_local_api_key
PINECONE_API_KEY=your_pinecone_key
PINECONE_INDEX_NAME=hackrx-index
GROQ_API_KEY=your_groq_api_key
```

---

## ğŸ§  Deduplication Logic

Each uploaded document is hashed using `SHA256`. This ensures:

* âœ… You never embed the same document twice
* âœ… Pinecone stores each chunk under an ID like:

```
{doc_hash}-chunk-{i}
```

If the same PDF is uploaded again, embedding is **skipped** and only retrieval happens.

---

## ğŸ§  Embedding Model (Local)

This app uses [HuggingFace Sentence Transformers](https://www.sbert.net/) locally
Fast and suitable for short to mid-length documents.

---

## ğŸ“ Sentence-Aware Chunking (NLTK)

Documents are first tokenized into sentences using NLTK's `sent_tokenize()`.

Chunks are then built using sentence boundaries (not blindly splitting by character length), improving semantic consistency.

**Overlap** between chunks is handled at sentence level for better context flow.


---

## ğŸ“Œ Chunk Splitting and ID Logic

* Chunk size: \~700 characters
* Overlap: 1â€“2 sentences from the previous chunk
* Chunk ID: `{doc_hash}-chunk-{i}`

This allows:

* ğŸ”„ Consistent re-chunking
* ğŸ§¼ Idempotent upserts into Pinecone

---

## ğŸ” Authorization

All API endpoints are protected via **Bearer Token**.
Provide this header in every request:

```http
Authorization: Bearer your_local_api_key
```

---

## ğŸ§ª Running Locally

### 1. Install requirements

```bash
pip install -r requirements.txt
```

### 2. Run the FastAPI server

```bash
uvicorn main1:app --reload
```

Then go to [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) to try the API with Swagger UI.

---

## ğŸ§¼ Optional: Clear Pinecone Index

To clear all document vectors (dev-only):

```bash
curl -X POST http://localhost:8000//hackrx/clear
```

---

## ğŸ§  Model Used

Answering is powered by:

> **LLaMA-3.1-70B Versatile**
> served via [Groq API](https://console.groq.com/) â€” ultra low-latency inference with OpenAI-compatible API interface.

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Acknowledgements

* [HuggingFace](https://huggingface.co/sentence-transformers) for MiniLM sentence encoders
* [Groq](https://groq.com/) for blazing-fast LLM inference
* [Pinecone](https://pinecone.io/) for scalable vector storage
* [NLTK](https://www.nltk.org/) for sentence-aware chunking
* [FastAPI](https://fastapi.tiangolo.com/) for building production APIs fast

---
